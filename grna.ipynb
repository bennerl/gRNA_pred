{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"These are the variables for the subsequent models\"\"\"\n",
    "classifier = 'series' # series or game\n",
    "stats_df = \"hockey_stats_train.txt\" #all the stats\n",
    "test_df = \"hockey_stats_2015.txt\" #2015 stats\n",
    "playoff = \"2015_playoffs.txt\" #playoff structure of 2015\n",
    "year = \"2015\" #test year\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def model_parameters(stats_df, classifier, test_df, playoff):\n",
    "    \"\"\"This function reads in the overall stats df and parses it into the \n",
    "    training data and then the classes of the training data. It scales the training\n",
    "    data for downstream analysis. It also reads in the test df and parses it into\n",
    "    the test data as well as its classifiers. It also scales the test data based on the \n",
    "    scaler transformation of the training data. It then returns the df's\n",
    "    for subsequent analysis.\"\"\"\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    stats_df = pd.read_csv(stats_df, sep = \"\\t\")\n",
    "    train_df = stats_df.iloc[:,4:len(stats_df.columns)]\n",
    "    scaler.fit(train_df)\n",
    "    train_df = scaler.transform(train_df)\n",
    "    test_df = pd.read_csv(test_df, sep = \"\\t\")\n",
    "    teams = test_df.iloc[:,0]\n",
    "    playoff_df = pd.read_csv(playoff, sep = \"\\t\", header = None)\n",
    "    if classifier == \"series\":\n",
    "        train_class = stats_df.iloc[:,2]\n",
    "        test_class = test_df.iloc[:,2]\n",
    "    elif classifier == \"game\":\n",
    "        train_class = stats_df.iloc[:,3]\n",
    "        test_class = test_df.iloc[:,3]\n",
    "    test_df = test_df.iloc[:,4:len(stats_df.columns)]\n",
    "    test_df = scaler.transform(test_df)\n",
    "        \n",
    "    return(train_df, train_class, test_df, test_class, teams, playoff_df)\n",
    "\n",
    "train_df, train_class, test_df, test_class, teams, playoff_df = model_parameters(stats_df, classifier, test_df, playoff)\n",
    "#print(train_df)\n",
    "#print(train_class)\n",
    "print(test_df)\n",
    "print(test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "def ML_neural_network(train_df, train_class, test_df, test_class):\n",
    "    \"\"\"This function reads in the training df and classifier and runs an exhaustive parameter search for multi-layer\n",
    "    perceptron classifier (neural network) with gridsearchcv(). It then proceeds to determine \n",
    "    the best set of parameters for multi-layer\n",
    "    perceptron classifier with the repeated stratified\n",
    "    Kfold cross validation and then applies the best parameters and predicts the classes on the test data set.\n",
    "    It finally returns the predicted classes on the test data set, probabilities \n",
    "    of belonging to the classes on the test data set, the number of correct classifiers on the test data set, \n",
    "    the best parameters, and the best score from the gridsearchcv().\n",
    "    \n",
    "    We decided to leave out solvers adam and sgd as they did not converge on the training data\n",
    "    and lbfgs was recomended for smaller datasets such as the one we have\"\"\"\n",
    "\n",
    "    parameters = {'solver':['lbfgs'], 'activation':['identity', 'logistic', 'tanh', 'relu'], 'alpha':[.1, .01, .001], 'max_iter':[500]}\n",
    "    kfold = RepeatedStratifiedKFold(n_splits=4, n_repeats=5) # the cross validator\n",
    "    ml = MLPClassifier() #the model we are running\n",
    "    clf = GridSearchCV(ml, parameters, cv = kfold, scoring='accuracy', n_jobs=-1)\n",
    "    clf.fit(train_df, train_class) #fits the best parameters to the training data\n",
    "    predict_df = clf.predict(test_df) #predicts the classes on the test df\n",
    "    probability_df = clf.predict_proba(test_df) #gets the probabilites of each instance belonging to the class on the test df\n",
    "    ml_score = clf.score(test_df, test_class) #calculates the score of correct classifications from predict\n",
    "    params = clf.best_params_ #best parameters from gridsearchcv()\n",
    "    bestscore = clf.best_score_ #best score from gridsearchcv()\n",
    "    return(predict_df, probability_df, ml_score, params, bestscore)\n",
    "\n",
    "predict_df, probability_df, ml_score, params, bestscore = ML_neural_network(train_df, train_class, test_df, test_class)\n",
    "\n",
    "print(\"The best parameters are:\", '\\n', params, '\\n')\n",
    "print(\"The score of the 2015 playoffs are: \", ml_score, '\\n')\n",
    "print(\"The best score from the CV are: \", bestscore, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def ML_logistic_regression(train_df, train_class, test_df, test_class):\n",
    "    \n",
    "    \"\"\"This function reads in the training df and classifier and runs an exhaustive parameter search for logistic regression\n",
    "    with gridsearchcv(). Note that the parameters for this logistic regression that are different from the other \n",
    "    logistic regression cells are:\n",
    "    \n",
    "    'multi_class':['ovr', 'multinomial']\n",
    "    'penalty':['l2']\n",
    "    'solver':['lbfgs', 'newton-cg']\n",
    "    \n",
    "    It then proceeds to determine the best set of parameters for logistic regression with the repeated stratified\n",
    "    Kfold cross validation and then applies the best parameters and predicts the classes on the test data set.\n",
    "    It finally returns the predicted classes on the test data set, probabilities \n",
    "    of belonging to the classes on the test data set, the number of correct classifiers on the test data set, \n",
    "    the best parameters, and the best score from the gridsearchcv().\n",
    "    \n",
    "    We decided to leave out solvers sag and saga as they did not converge on the training data and are not recomended\n",
    "    for smaller datasets such as ours. The solvers we used in this class can onyl use l2 penalty.\"\"\"\n",
    "\n",
    "    parameters = {'multi_class':['ovr', 'multinomial'], 'penalty':['l2'], 'C':[.1, 1.0, 10], 'solver':['lbfgs', 'newton-cg'], 'class_weight':['balanced'], 'max_iter':[500]}\n",
    "    kfold = RepeatedStratifiedKFold(n_splits=4, n_repeats=5) #the cross validator\n",
    "    ml = LogisticRegression() #the model we are running\n",
    "    clf = GridSearchCV(ml, parameters, cv = kfold, scoring = 'accuracy', n_jobs=-1)\n",
    "    clf.fit(train_df, train_class) #fits the best parameters to the training data\n",
    "    predict_df = clf.predict(test_df) #predicts the classes on the test df\n",
    "    probability_df = clf.predict_proba(test_df) #gets the probabilites of each instance belonging to the class on the test df\n",
    "    ml_score = clf.score(test_df, test_class) #calculates the score of correct classifications from predict\n",
    "    params = clf.best_params_ #best parameters from gridsearchcv()\n",
    "    bestscore = clf.best_score_ #best score from gridsearchcv()\n",
    "    return(predict_df, probability_df, ml_score, params, bestscore)\n",
    "\n",
    "predict_df, probability_df, ml_score, params, bestscore = ML_logistic_regression(train_df, train_class, test_df, test_class)\n",
    "\n",
    "print(\"The best parameters are:\", '\\n', params, '\\n')\n",
    "print(\"The score of the 2015 playoffs are: \", ml_score, '\\n')\n",
    "print(\"The best score from the CV are: \", bestscore, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
